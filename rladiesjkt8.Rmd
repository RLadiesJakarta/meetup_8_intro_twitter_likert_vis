---
title: "R-Ladies Jakarta 8th Meetup: Intro to Twitter Mining in R"
author: "Erika Siregar [&lt;&#64;erikaris&gt;](https://twitter.com/erikaris)"
date: "7/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## How to get Twitter Data {.build}

- Step 1: Setup your account
- Step 2: Download/extract data
- Step 3: Analyze and visualize


## Setting Up Your Twitter Account

Please refer to [https://github.com/RLadiesJakarta/meetup_8_intro_twitter_likert_vis/blob/master/settingup_twitter_account.md](https://github.com/RLadiesJakarta/meetup_8_intro_twitter_likert_vis/blob/master/settingup_twitter_account.md)

## Obtaining Data from Twitter

### Prepare the Library

```{r results=FALSE, message=FALSE, warning=FALSE}
library(rtweet)        # R client for accessing Twitter's REST and stream APIs
library(wordcloud)     # for creating wordcloud
library(dplyr)         # dplyr
library(igraph)        # for creating network graph
library(tm)            # for text mining
library(RColorBrewer)  # for coloring things
library(qdapRegex)       # for regex
library(qdap)
library(maps)
```

### Create Oauth (Twitter API) token. 

```{r eval=FALSE}
# create token named 'mytoken' 
mytoken <- create_token( app = "mytwitterapp", # you can change the app name 
                         consumer_key = "your consumer key", 
                         consumer_secret = "your consumer secret", 
                         access_token = "your access token", 
                         access_secret = "your access token secret" 
                         )
```

Example:

```{r results='hide', error=FALSE, message=FALSE}
# create token named 'mytoken' 
mytoken <- create_token( app = "rladiesjkt", # you can change the app name 
                         consumer_key = "rqVT4hxIZnSmsxy8ZevGfPjdQ", 
                         consumer_secret = "FYwwjBygIwjHQP52q0uDQHqgNABLVIeEgRb0MFelQoXeaRzsZ9", 
                         access_token = "284205655-TYp8cJPo45g5dseIAwtbaECS5DwJGaPpCYc1hAjI", 
                         access_secret = "Qb7mzFoAog9jRkNBUIxUJGjIiShC3d8nTlZ7VW60uIWkJ" 
                         )
```

### Collecting The Tweets
  
There are two function that we can choose in order to acquire twitter data: 

1. `stream_tweets()`: catches the tweet for current checkpoint until timeout, so with `stream_tweets()` we are moving forward and obtain the most up-to-date tweets.
2. `search_tweets()`: from the current checkpoint backward.

### Stream Tweets
Usage: 
```{r eval=FALSE}
my_tweets = stream_tweets(q = "", timeout = 30, token = NULL)
```

Notes:

1. q = "keyword", can be a keyword or screen name (up to 400 keywords, comma separated). Example: `stream_tweets("covid19", timeout = 60)`, `stream_tweets("erikaris", timeout = 90)`
2. timeout = define how long the tweet streaming will last (in seconds). To stream indefinitely, use timeout = FALSE.
3. token = the Oauth (Twitter API) token created on the previous step.

The `stream_tweets()` returns a dataframe consist of `number_of_tweet` rows and 90 columns. 

![](images/stream_tweets_df.png).

Example:
```{r message=FALSE, warning=FALSE}
tweet60 <- stream_tweets("jokowi", timeout = 60, token = mytoken)
head(tweet60)
```

### Search Tweets

`search_tweets()` returns a maximum of *18,000* tweets for each request posted. 
Usage: `search_tweets(q, n = 100, include_rts = FALSE)`.

Notes:

1. q = queryÂ  of the tweets to be searched. 
2. n = integer --> number of desired tweets to download.
3. include_rts = logical --> if TRUE, it means include retweets in search results.

The `search_tweets()` returns a dataframe similar to that of generated by `stream_tweets()`.

Example:
```{r message=FALSE, warning=FALSE}
jokowi <- search_tweets("jokowi", n = 10000, lang = "id")
head(jokowi)
```

### Extracting User Information

Usage: `users_data(tweets)`.

Notes: tweets is a tweet dataframe obtained from the collecting tweets done in the previous step. 

This will return a tibble dataframe consists of n rows (observations) and 20 columns (variables). <br />
The number of rows (n) will be equal to the number of users involved in the collected tweet object. <br /> 

Example:

```{r message=FALSE, warning=FALSE}
usrdt <- users_data(jokowi)
head(usrdt)
names(usrdt)
```

## Analyzing the Tweets (Mari Berkepo Ria)

### Find Users Who Tweets the Most about Certain Topics

```{r message=FALSE, error=FALSE}

# step 1: Create a table of users and tweet counts for the topic
jokowi_user <- table(jokowi$screen_name)

# step 2: sort the table in descending order of tweet counts
jokowi_user_sort <- sort(jokowi_user, decreasing = TRUE)

# step 3: View sorted table for top 10 users
head(jokowi_user_sort, 10)
```

### Compare The Number of Followers between Certain Users

To do this, we take advantage of using `lookup_users()` function.

```{r message=FALSE, warning=FALSE}
# get the data of intended users
mypeople <- lookup_users(c("erikaris", "ulfahregar", "rladiesjakarta", "akkuderry", "gwangge", "ikakarlina", "r_indonesia_", "rladiesbogor"))

# take only columns "screen_name", "followers_count"
mypeople_count <- mypeople[, c("screen_name", "followers_count")]

# order by followers_count (decreasing)
mypeople_count <- mypeople_count[order(-mypeople_count$followers_count), ]

mypeople_count
```

### Counting The Number of Retwets

```{r message=FALSE, warning=FALSE}
library(dplyr)

rtwt <- jokowi[, c("text", "retweet_count")]
rtwt_sort <- arrange(rtwt, desc(retweet_count))
head(rtwt_sort)
```


## Analyzing Twitter Trend

### Get The Trend

```{r message=FALSE, warning=FALSE}
trend_jkt <- get_trends("Jakarta")
head(trend_jkt)
```

### Visualize the Trend

```{r message=FALSE, error=FALSE}
ts_plot(jokowi, by = 'mins', color = 'red')
```

### Counting frequency of tweets over a specified time interval

```{r message=FALSE, error=FALSE}
ts_data(jokowi, by = "hours")
```

## Mining the Twitter Text

1. Extract the text only from the tweets dataset
  ```{r message = FALSE, error = FALSE}
  head(jokowi$text)
  ```
  
2. Remove the URLs from the text. 
Use function `rm_twitter_url()` from library qdapRegex. 

  ```{r message=FALSE, error=FALSE}
  jokowi_textrm <- rm_twitter_url(jokowi$text)
  head(jokowi_textrm)
  ```
  
3. Remove special characters, punctuation, & numbers from the output of the previous step. Use the function `gsub(pattern)`.

  ```{r message=FALSE, error=FALSE}
  jokowi_text_gsub <- gsub("[^A-Za-z]"," " , jokowi_textrm)
  head(jokowi_text_gsub)
  str(jokowi_text_gsub)
  ```
  
4. Create corpus 

In library `tm`, text must be converted into a corpus object so that we can apply various treatments on it later, such as lowercasing or stripping whitespace. 

```{r eval=FALSE, message=FALSE, error=FALSE} 
library("tm")
# Convert text in "twt_gsub" dataset to a text corpus and view output
jokowi_corpus <- jokowi_text_gsub %>% 
                  VectorSource() %>%  # change the list of elements into vector's document
                  Corpus()     # turn the vector into a corpus

head(jokowi_corpus)
head(jokowi_corpus$content)
```

5. Lower case the text

```{r message=FALSE, error=FALSE} 
library("tm")
# Convert text in "twt_gsub" dataset to a text corpus and view output
jokowi_corpus <- jokowi_text_gsub %>% 
                  VectorSource() %>%  # change the list of elements into vector's document
                  Corpus() %>%        # turn the vector into a corpus
                  tm_map(tolower) 

head(jokowi_corpus$content)   # focus on the corpus$content
```

6. Stripping whitespace

```{r message=FALSE, error=FALSE} 
library("tm")
# Convert text in "twt_gsub" dataset to a text corpus and view output
jokowi_corpus <- jokowi_text_gsub %>% 
                  VectorSource() %>%  # change the list of elements into vector's document
                  Corpus() %>%        # turn the vector into a corpus
                  tm_map(tolower) %>%  # lower case the text
                  tm_map(stripWhitespace)  # remove unnecessary whitespace

head(jokowi_corpus$content)   # focus on the corpus$content
```

7. remove stopwords. 

Stop words are commonly used words that are excluded from searches to help index and parse web pages faster. <br />
Available stopwords library are mostly for English language. <br />
There is [Sastrawi](http://sastrawi.github.io/) for Indonesian but it's for python and hasn't been updated for 4 years. <br />
Create our own stopwords library. 

```{r message=FALSE, warning=FALSE}
library("tm")

# taken from https://github.com/sastrawi/sastrawi/blob/master/src/Sastrawi/StopWordRemover/StopWordRemoverFactory.php

custom_stopwds <- c('yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti', 'jika', 'jika', 'sehingga', 'kembali', 'dan', 'tidak', 'ini', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'sementara', 'setelah', 'belum', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bisa', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'tanpa', 'agak', 'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'ingin', 'juga', 'nggak', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun')

# Convert text in "twt_gsub" dataset to a text corpus and view output
jokowi_corpus <- jokowi_text_gsub %>% 
                  VectorSource() %>%  # change the list of elements into vector's document
                  Corpus() %>%        # turn the vector into a corpus
                  tm_map(tolower) %>%  # lower case the text
                  tm_map(stripWhitespace) %>%  # remove unnecessary whitespace
                  tm_map(removeWords, custom_stopwds)  # remove stopwords

head(jokowi_corpus$content)   # focus on the corpus$content
```

## Visualization

### Bar Graph

Top 10 Words

```{r message=FALSE, warning=FALSE}

library(qdap)

# Extract term frequencies for the top 10 words
termfreq10 <- freq_terms(jokowi_corpus$content, 10) 
termfreq10

# Create a bar plot using terms with more than 60 counts
ggplot(termfreq10, aes(x = reorder(WORD, -FREQ), y = FREQ)) + 
		geom_bar(stat = "identity", fill = "red") + 
		theme(axis.text.x = element_text(angle = 15, hjust = 1))
```

### Wordcloud

```{r message=FALSE, warning=FALSE}
library(wordcloud)
# Create a word cloud in red with min frequency of 20
wordcloud(jokowi_corpus$content, min.freq = 20, colors = "red", 
    scale = c(3,0.5),random.order = FALSE)
```

#### Fancy for some colors?

```{r message=FALSE, warning=FALSE}
library(wordcloud)
library(RColorBrewer)

# Create word cloud with 6 colors and max 50 words
wordcloud(jokowi_corpus$content, max.words = 50, 
    colors = brewer.pal(6, "Dark2"), 
    scale=c(4,1), random.order = FALSE)
```

## When in doubt, see it on map

For now, the `maps()` package can only support few regions: world, states, nz, france, italy, and canada. <br />

Indonesia ???

```{r message=FALSE, warning=FALSE}

library(maps)

# Extract geo-coordinates data to append as new columns
jokowi_coord <- lat_lng(jokowi)

# View the columns with geo-coordinates for first 20 tweets
head(jokowi_coord[c("lat","lng")], 20)

jokowi_geo <- na.omit(jokowi_coord[,c("lat", "lng")])

# Plot longitude and latitude values of tweets on the US state map
map(database = 'world', fill = TRUE, col = 'light yellow')
with(jokowi_geo, points(lng, lat, pch = 20, cex = 1, col = 'blue'))

```